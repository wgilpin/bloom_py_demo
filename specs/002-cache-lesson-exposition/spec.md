# Feature Specification: Cache Lesson Exposition

**Feature Branch**: `002-cache-lesson-exposition`  
**Created**: 2025-11-25  
**Status**: Draft  
**Input**: User description: "the initial lesson exposition offered in the chat after topic selection is generated anew each time. This is wasteful. Add cacheing to the exposition"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Repeated Topic Study with Cached Exposition (Priority: P1)

A student selects a subtopic they've studied before (or that any student has studied before). The system displays the same high-quality exposition that was generated previously instead of making a new LLM call. The exposition content is identical for all students studying the same subtopic, providing consistency and reducing API costs.

**Why this priority**: This is a cost optimization that directly addresses wasteful API usage. Every repeated topic selection currently generates an identical exposition, multiplying costs unnecessarily. Caching provides immediate cost savings while maintaining the same user experience.

**Independent Test**: Select a subtopic (e.g., "Fractions"), note the exposition content, exit the session, select the same subtopic again, and verify the exposition is identical without delay. Check application logs to confirm no LLM API call was made for the cached exposition.

**Acceptance Scenarios**:

1. **Given** a subtopic exposition has never been generated before, **When** a student starts a session on that subtopic, **Then** the system generates the exposition via LLM and stores it in the cache
2. **Given** a subtopic exposition exists in the cache, **When** a student starts a session on that subtopic, **Then** the system retrieves the cached exposition without calling the LLM API
3. **Given** a subtopic exposition is retrieved from cache, **When** the student reads it, **Then** the content quality and format are indistinguishable from a freshly generated exposition
4. **Given** two students (or the same student twice) study the same subtopic, **When** both sessions start, **Then** both receive the exact same cached exposition text
5. **Given** the cache stores expositions for multiple subtopics, **When** a student switches between subtopics, **Then** each subtopic displays its correct cached exposition

---

### User Story 2 - First-Time Exposition Generation (Priority: P1)

A student is the first person to study a particular subtopic (or studies it after the cache has been cleared). The system generates the exposition via LLM as it does currently, stores it for future use, and displays it to the student.

**Why this priority**: This ensures the caching mechanism doesn't break existing functionality. First-time generation must work seamlessly, with caching happening transparently in the background.

**Independent Test**: Clear all cached expositions, select a subtopic that has no cache entry, verify the exposition is generated successfully and displays normally, then verify the exposition is now cached for future use.

**Acceptance Scenarios**:

1. **Given** the cache is empty for a subtopic, **When** a student starts a session on that subtopic, **Then** the system generates the exposition via LLM within the normal 3-second response time
2. **Given** the exposition generation completes successfully, **When** the content is displayed to the student, **Then** the system automatically saves it to the cache associated with that subtopic
3. **Given** the exposition was just generated and cached, **When** another student selects the same subtopic immediately after, **Then** they receive the newly cached version without regeneration
4. **Given** the LLM API fails during first-time generation, **When** the error occurs, **Then** the system displays the standard error message and does not store an incomplete or error state in the cache

---

### Edge Cases

- What happens if the cache becomes corrupted or contains invalid data for a subtopic? System detects invalid cache entries, regenerates the exposition, and overwrites the corrupted cache entry
- What if an admin wants to refresh the cached exposition for a subtopic (e.g., to improve quality)? Admin can clear specific cache entries or the entire cache, forcing regeneration on next access
- How large can the cache grow if there are hundreds of subtopics? Cache size is bounded by the number of subtopics in the syllabus (reasonable limit for GCSE math, typically 50-100 subtopics); each exposition is ~500-1000 tokens of text
- What if the LLM model or provider changes (e.g., switching from GPT-4 to Claude)? The system stores the model identifier with each cached exposition, allowing admins to selectively invalidate cache entries generated by the old model, or keep both versions for comparison
- What if the exposition prompt or LLM behavior changes in a future update? Cache entries include a version identifier or timestamp, allowing the system to invalidate old cache entries when the generation logic changes
- What happens if two students simultaneously start sessions on the same uncached subtopic? The system handles concurrent generation gracefully, potentially generating twice but storing one result, or using database locking to prevent duplicate generation
- Can cached expositions be exported or migrated to another instance of the system? Cache entries are stored in the database and can be backed up or transferred along with the syllabus data

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST store generated lesson expositions in persistent storage (database) indexed by subtopic ID
- **FR-002**: System MUST check for a cached exposition before calling the LLM API when starting a new tutoring session
- **FR-003**: System MUST retrieve and display the cached exposition if one exists for the requested subtopic
- **FR-004**: System MUST generate a new exposition via LLM only when no cached version exists for the requested subtopic
- **FR-005**: System MUST automatically save newly generated expositions to the cache immediately after successful generation, including metadata: generation timestamp and the LLM model identifier used
- **FR-006**: System MUST ensure cached expositions are associated with the correct subtopic ID to prevent mismatches
- **FR-007** `[LATER]`: System MUST validate cached exposition data before using it (e.g., non-empty, properly formatted text)
- **FR-008** `[LATER]`: System MUST regenerate and re-cache an exposition if the cached version is invalid or corrupted
- **FR-009** `[LATER]`: System SHOULD provide an admin function to clear the exposition cache for a specific subtopic or all subtopics
- **FR-010** `[LATER]`: System SHOULD log when expositions are served from cache vs. newly generated for monitoring and cost tracking
- **FR-011** `[LATER]`: System MAY include a cache version identifier to support future cache invalidation when exposition generation logic changes

### Key Entities *(include if feature involves data)*

- **Cached Exposition**: A stored lesson exposition for a specific subtopic. Contains the full text content of the exposition, the subtopic ID it belongs to, generation timestamp (when it was created), the LLM model identifier that generated it (e.g., "gpt-4", "claude-3-5-sonnet-20241022"), and optionally a cache version identifier. Stored in the database and retrieved on-demand during session initialization. The model identifier enables cache invalidation when switching LLM providers or models.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: For subtopics with cached expositions, session start time decreases from ~3 seconds to under 0.5 seconds (measured from topic selection to exposition display)
- **SC-002**: LLM API calls for exposition generation are reduced by at least 90% in typical usage where students revisit topics or multiple students study the same topics
- **SC-003**: 100% of cached expositions display correctly without formatting errors or content corruption
- **SC-004**: In testing with 20 subtopics, after initial generation, 100% of subsequent sessions retrieve cached expositions successfully
- **SC-005**: API cost for exposition generation decreases proportionally to cache hit rate (e.g., 90% cache hit rate = ~90% cost reduction for expositions)
- **SC-006**: Students cannot distinguish between cached and freshly generated expositions in quality or formatting in blind A/B testing

## Assumptions

- Exposition content can be standardized per subtopic—it doesn't need to be personalized per student or vary over time within the same application instance
- The exposition generation prompt and LLM configuration remain stable—if changes are made, the cache can be manually cleared to force regeneration, with the model identifier helping identify which cache entries need refreshing
- Storage space for cached expositions is negligible compared to other data (each exposition is ~500-1000 tokens of text, or 2-4KB, plus ~50 bytes for metadata including timestamp and model identifier; 100 subtopics = ~200-400KB total)
- Cached expositions are shared across all students using the system (single-instance demo)—no per-student customization required
- Cache invalidation is manual for MVP—no automatic expiration or time-based refresh needed
- Cache will be stored in the same SQLite database as other application data
- The existing exposition generation logic in `exposition_node` will be wrapped with cache-check-first logic
- Cache miss on first access per subtopic is acceptable—the first student to study a topic pays the generation cost, subsequent students benefit from the cache
- The admin interface for cache management can be simple (e.g., SQL query, admin page button, or command-line script)

